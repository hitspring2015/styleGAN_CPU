--- build_module_src_orgin.py	2019-06-23 02:25:56.590196900 +0800
+++ build_module_src.py	2019-06-24 10:47:48.493178600 +0800
@@ -11,6 +11,7 @@
 import tensorflow as tf
 import dnnlib
 import dnnlib.tflib as tflib
+import sys,ipdb
 
 # NOTE: Do not import any application-specific modules here!
 # Specify all network parameters as kwargs.
@@ -41,10 +42,18 @@
 
     # Convolve using depthwise_conv2d.
     orig_dtype = x.dtype
+    
     x = tf.cast(x, tf.float32)  # tf.nn.depthwise_conv2d() doesn't support fp16
     f = tf.constant(f, dtype=x.dtype, name='filter')
-    strides = [1, 1, stride, stride]
-    x = tf.nn.depthwise_conv2d(x, f, strides=strides, padding='SAME', data_format='NCHW')
+    #strides = [1, 1, stride, stride]
+    strides = [1, stride, stride, 1]
+    
+    x = tf.transpose(x, [0,2,3,1], name='NCHW_to_NHWC')
+    #f = tf.transpose(f, [0,2,3,1], name='NCHW_to_NHWC') 
+	# NCHW patchs
+    x = tf.nn.depthwise_conv2d(x, f, strides=strides, padding='SAME', data_format='NHWC')
+    
+    x = tf.transpose(x, [0,3,1,2], name='NHWC_to_NCHW')
     x = tf.cast(x, orig_dtype)
     return x
 
@@ -87,7 +96,14 @@
     # Large factor => downscale using tf.nn.avg_pool().
     # NOTE: Requires tf_config['graph_options.place_pruned_graph']=True to work.
     ksize = [1, 1, factor, factor]
-    return tf.nn.avg_pool(x, ksize=ksize, strides=ksize, padding='VALID', data_format='NCHW')
+    #ksize = [1, factor, factor, 1]
+    # <tf.Tensor 'D/images_in:0' shape=(?, 3, 512, 512) dtype=float32>
+    # <tf.Tensor 'D/NHWC_to_NCHW:0' shape=(?, 3, 512, 256) dtype=float32>
+    #x = tf.transpose(x, [0,2,3,1], name='NCHW_to_NHWC')
+    # NCHW patchs
+    pool = tf.nn.avg_pool(x, ksize=ksize, strides=ksize, padding='VALID', data_format='NCHW')
+    #return tf.transpose(pool, [0,3,1,2], name='NHWC_to_NCHW')
+    return pool
 
 #----------------------------------------------------------------------------
 # High-level ops for manipulating 4D activation tensors.
@@ -127,6 +143,7 @@
                 dx = _upscale2d(dy, factor, gain=1/factor**2)
                 return dx, lambda ddx: _downscale2d(ddx, factor)
             return y, grad
+        #ipdb.set_trace()
         return func(x)
 
 #----------------------------------------------------------------------------
@@ -163,9 +180,16 @@
 
 def conv2d(x, fmaps, kernel, **kwargs):
     assert kernel >= 1 and kernel % 2 == 1
+    
+    
     w = get_weight([kernel, kernel, x.shape[1].value, fmaps], **kwargs)
     w = tf.cast(w, x.dtype)
-    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME', data_format='NCHW')
+    # patchs
+    x = tf.transpose(x, [0,2,3,1], name='NCHW_to_NHWC')
+    #w = tf.transpose(w, [0,2,3,1], name='NCHW_to_NHWC')
+    conv = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME', data_format='NHWC')
+    return tf.transpose(conv, [0,3,1,2], name='NHWC_to_NCHW')
+    #return conv
 
 #----------------------------------------------------------------------------
 # Fused convolution + scaling.
@@ -186,9 +210,21 @@
     w = tf.transpose(w, [0, 1, 3, 2]) # [kernel, kernel, fmaps_out, fmaps_in]
     w = tf.pad(w, [[1,1], [1,1], [0,0], [0,0]], mode='CONSTANT')
     w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]])
+    
     w = tf.cast(w, x.dtype)
-    os = [tf.shape(x)[0], fmaps, x.shape[2] * 2, x.shape[3] * 2]
-    return tf.nn.conv2d_transpose(x, w, os, strides=[1,1,2,2], padding='SAME', data_format='NCHW')
+    #os = [tf.shape(x)[0], fmaps, x.shape[2] * 2, x.shape[3] * 2]
+    os = [tf.shape(x)[0], x.shape[2] * 2, x.shape[3] * 2, fmaps]
+    
+    # patchs
+    x = tf.transpose(x, [0,2,3,1], name='NCHW_to_NHWC')
+    #os = tf.transpose(os, [0,3,1,2], name='NHWC_to_NCHW')
+    #w = tf.transpose(w, [0,2,3,1], name='NCHW_to_NHWC')
+    # patchs
+    #return tf.nn.conv2d_transpose(x, w, os, strides=[1,1,2,2], padding='SAME', data_format='NHWC')
+    #ipdb.set_trace()
+    conv = tf.nn.conv2d_transpose(x, w, os, strides=[1,2,2,1], padding='SAME', data_format='NHWC')
+    return tf.transpose(conv, [0,3,1,2], name='NHWC_to_NCHW')
+    #return conv
 
 def conv2d_downscale2d(x, fmaps, kernel, fused_scale='auto', **kwargs):
     assert kernel >= 1 and kernel % 2 == 1
@@ -205,7 +241,14 @@
     w = tf.pad(w, [[1,1], [1,1], [0,0], [0,0]], mode='CONSTANT')
     w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]]) * 0.25
     w = tf.cast(w, x.dtype)
-    return tf.nn.conv2d(x, w, strides=[1,1,2,2], padding='SAME', data_format='NCHW')
+    # patchs
+    #return tf.nn.conv2d(x, w, strides=[1,1,2,2], padding='SAME', data_format='NHWC')
+    #w = tf.transpose(w, [0,2,3,1], name='NCHW_to_NHWC')
+    x = tf.transpose(x, [0,2,3,1], name='NCHW_to_NHWC')
+    conv = tf.nn.conv2d(x, w, strides=[1,2,2,1], padding='SAME', data_format='NHWC')
+    return tf.transpose(conv, [0,3,1,2], name='NHWC_to_NCHW')
+    #return conv
+
 
 #----------------------------------------------------------------------------
 # Apply bias to the given activation tensor.
@@ -631,6 +674,7 @@
         for res in range(resolution_log2, 2, -1):
             lod = resolution_log2 - res
             x = block(x, res)
+            #ipdb.set_trace()
             img = downscale2d(img)
             y = fromrgb(img, res - 1)
             with tf.variable_scope('Grow_lod%d' % lod):
